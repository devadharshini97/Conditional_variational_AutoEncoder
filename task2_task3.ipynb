{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebADatasetPreprocessing(Dataset):\n",
    "    def __init__(self, img_dir, attr_path, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.attr_path = attr_path\n",
    "        self.transform = transform\n",
    "        self.attr_list = []\n",
    "        self.attr_list = self.get_attributes()\n",
    "        self.task = []\n",
    "        self.task = self.task_2()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.attr_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f'{self.img_dir}/{idx+1:06}.jpg' #inputs the first image path\n",
    "        image = Image.open(img_path).convert('RGB') #opens the first image and stores it as RGB\n",
    "        #print(image.size)\n",
    "        attribute_vectors = torch.tensor(self.attr_list[idx], dtype=torch.float32)\n",
    "        task2_changed_attributes = torch.tensor(self.task[idx], dtype=torch.float32)\n",
    "        #converts the attribute vector to a tensor\n",
    "        if self.transform:\n",
    "             image = self.transform(image) #transforms the image based on the defined transformation\n",
    "        fc_layer = torch.nn.Linear(40,64*64)(attribute_vectors) \n",
    "        #passing the tensor attributes through a fully connected layer:o/p dimension is (64*64)\n",
    "        reshaped_attr = fc_layer.view(1,64,64) \n",
    "        #reshaped again to dimensions(1,64,64)\n",
    "        attribute_channel = torch.zeros_like(image[0])+reshaped_attr\n",
    "        image = torch.cat([image,attribute_channel],dim=0)\n",
    "        #image(3,64,64) and the attribute (1,64,64) concatenated and gives output ->(4,64,64)\n",
    "        return image, attribute_vectors, task2_changed_attributes #image(4,64,64) and attributevector tensor(40)\n",
    "    \n",
    "    def get_attributes(self):\n",
    "        attr_list = []\n",
    "        with open(self.attr_path, 'r') as f: #opens the attribute txt file\n",
    "            lines = f.readlines() #stores each line as a list\n",
    "            for i, line in enumerate(lines): #[(0,'element1 from lines list'),(1,'element1 from lines list'),....]\n",
    "                if i < 2:\n",
    "                    continue  #removes the string elements[(2,'img1 -1 1...40...')]\n",
    "                attr = line.split()[1:] #splits to 'img1 -1 1...40...' and ignores img1 and stores from -1 1...40... as string\n",
    "                attr = [int(a) for a in attr] #converts string to integer and stores as list -> [...40..]\n",
    "                attr_list.append(attr) #appends each list \n",
    "        return attr_list #returns shape [[...40..],[...40...],[...40...],...2Lakhs]\n",
    "    \n",
    "    def task_2(self):\n",
    "        attr_list = []\n",
    "        with open(self.attr_path, 'r') as f: #opens the attribute txt file\n",
    "            lines = f.readlines() #stores each line as a list\n",
    "            for i, line in enumerate(lines): #[(0,'element1 from lines list'),(1,'element1 from lines list'),....]\n",
    "                if i < 2:\n",
    "                    continue  #removes the string elements[(2,'img1 -1 1...40...')]\n",
    "                attr = line.split()[1:] #splits to 'img1 -1 1...40...' and ignores img1 and stores from -1 1...40... as string\n",
    "                attr = [int(a) for a in attr] #converts string to integer and stores as list -> [...40..]\n",
    "                if attr[15] ==-1:   #by changing values of 31,22,15 we get different outputs\n",
    "                    attr[15]=1\n",
    "                # else:\n",
    "                #     attr[22]=1\n",
    "                attr_list.append(attr) #appends each list \n",
    "        return attr_list #returns shape [[...40..],[...40...],[...40...],...2Lakhs]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CelebADatasetPreprocessing('D:/sem2/adv_ML/HW3/dataset/image_dataset', 'D:/sem2/adv_ML/HW3/dataset/list_attr_celeba_image_dataset.txt',transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.90 * len(dataset)), len(dataset) - int(0.90 * len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4500, 500)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader) #500/32 = 15.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) #4500/32 = 140.625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.fc_mu = nn.Linear(512 * 2 * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 * 2 * 2, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.leaky_relu(self.conv3(x))\n",
    "        x = F.leaky_relu(self.conv4(x))\n",
    "        x = F.leaky_relu(self.conv5(x))\n",
    "        x = x.view(-1,512*2*2)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        # print(\"encoder output shape\")\n",
    "        # print(x.shape)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        z = torch.randn_like(std) * std + mu\n",
    "        return mu, z, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim + 40, 512 * 2 * 2)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.conv = nn.Conv2d(32, 3, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, zc):\n",
    "        #x = torch.cat([z, c], dim=1)\n",
    "        x = zc\n",
    "        x = self.fc1(x)\n",
    "        x = x.view(-1, 512, 2, 2)\n",
    "        x = F.leaky_relu(self.deconv1(x))\n",
    "        x = F.leaky_relu(self.deconv2(x))\n",
    "        \n",
    "        x = F.leaky_relu(self.deconv3(x))\n",
    "        x = F.leaky_relu(self.deconv4(x))\n",
    "        x = F.leaky_relu(self.deconv5(x))\n",
    "        x = torch.tanh(self.conv(x))\n",
    "        # print(\"decoder output shape\")\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not run this below model training section for task2 and task3. Because I am using pretrained model for these two tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import matplotlib.pyplot as plt\n",
    "encoder = Encoder(latent_dim=128).to(device)\n",
    "decoder = Decoder(latent_dim=128).to(device)\n",
    "enc_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "dec_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "def loss_function(decoded_image,x):\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(decoded_image[:,:,:,:], x[:,0:3,:,:])\n",
    "    rmse = torch.sqrt(loss) \n",
    "    return rmse\n",
    "num_epochs =5\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x,c,task2) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        enc_optimizer.zero_grad()\n",
    "        mean, z, encoded_image = encoder(x)\n",
    "        # grid1 = vutils.make_grid(encoded_image, nrow=8, normalize=True, scale_each=True)\n",
    "        # image1 = grid1.cpu().numpy().transpose((1, 2, 0))\n",
    "        # plt.imshow(image1)\n",
    "        # plt.show()\n",
    "        zc_concat = torch.cat([z,c],dim =1) # z -> (batch_size,latent), c -> (batch_size,40)\n",
    "        #zc ->(batch_size, latent+40)\n",
    "        dec_optimizer.zero_grad()\n",
    "        decoded_image = decoder(zc_concat)\n",
    "        loss = loss_function(decoded_image, x)\n",
    "        loss.backward()\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.10f}\")\n",
    "    torch.save(encoder.state_dict(), f\"D:/sem2/adv_ML/HW3/saved_models/trial1/encoder_celeba_{epoch+1}.pth\")\n",
    "    torch.save(decoder.state_dict(), f\"D:/sem2/adv_ML/HW3/saved_models/trial1/decoder_celeba_{epoch+1}.pth\")\n",
    "    grid2 = vutils.make_grid(decoded_image, nrow=8, normalize=True, scale_each=True)\n",
    "    image2 = grid2.cpu().numpy().transpose((1, 2, 0))\n",
    "    plt.imshow(image2)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('D:/sem2/adv_ML/HW3/saved_models/trial1/encoder_celeba_4.pth'))\n",
    "decoder.load_state_dict(torch.load('D:/sem2/adv_ML/HW3/saved_models/trial1/decoder_celeba_4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.utils as vutils\n",
    "for i, (x,c,task2) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        c = c.to(device)\n",
    "        task2 = task2.to(device)\n",
    "        mean, z, task2_encoded_image = encoder(x)\n",
    "        ztask2_concat = torch.cat([z,task2],dim =1) #task2 gives the manipulated attribute --> this is changed in the preprocessing class\n",
    "        task2_decoded_image = decoder(ztask2_concat)\n",
    "        vutils.save_image(task2_decoded_image, f\"D:/sem2/adv_ML/HW3/task2_images/glasses_images_{i+1}.png\", nrow=8)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('D:/sem2/adv_ML/HW3/saved_models/trial1/encoder_celeba_4.pth'))\n",
    "decoder.load_state_dict(torch.load('D:/sem2/adv_ML/HW3/saved_models/trial1/decoder_celeba_4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanlist =[]\n",
    "zlist =[]\n",
    "clist = []\n",
    "for i, (x,c,task2) in enumerate(test_loader):\n",
    "    x = x.to(device)\n",
    "    c = c.to(device)\n",
    "    mean, z, task2_encoded_image = encoder(x)\n",
    "    meanlist.append(mean)\n",
    "    zlist.append(z)\n",
    "    clist.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1_mean = meanlist[0] #tensor having 32 image means  #size (32,latent) i.e (32,128)\n",
    "batch2_mean = meanlist[1] #tensor having next 32 image means #size (32,latent) i.e (32,128)\n",
    "batch1_z = zlist[0]\n",
    "batch2_z = zlist[1]\n",
    "batch1_c = clist[0]\n",
    "batch2_c = clist[1]\n",
    "#add_c = batch1_c+batch2_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of images that morph from batch1 to batch2\n",
    "num_steps = 10\n",
    "for i in range(num_steps):\n",
    "    # Compute the interpolation vector\n",
    "    delta = i / (num_steps - 1)\n",
    "    mu_interp = (1 - delta) * batch1_mean + delta * batch2_mean\n",
    "    ztask3_concat = torch.cat([mu_interp,batch1_c],dim =1) #since moving from batch1 to batch2 images we choose the batch1 attribute vector\n",
    "    #can choose batch2 attribute vector to move from batch2 to batch1\n",
    "    # with torch.no_grad():\n",
    "    img_interp = decoder(ztask3_concat)\n",
    "    img_interp = (img_interp + 1) / 2.0  # Denormalize\n",
    "    #img_interp = transforms.ToPILImage()(img_interp)  # Convert to PIL image\n",
    "    vutils.save_image(img_interp, f\"D:/sem2/adv_ML/HW3/task3_images/transitioned_{i+1}.png\", nrow=8)\n",
    "    #img_interp.save(f'path/to/interpolated_{i}.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
